{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training+Evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5Q87hNYMZZj",
        "outputId": "1450e8cb-5726-454c-c3c0-462fa6f46b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ConvLSTM-Surgical-Tool-Tracker'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 76 (delta 33), reused 54 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n",
            "/content/ConvLSTM-Surgical-Tool-Tracker\n",
            "Repo cloned and extracted ...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CAMMA-public/ConvLSTM-Surgical-Tool-Tracker.git #contains model architecture\n",
        "%cd ConvLSTM-Surgical-Tool-Tracker\n",
        "\n",
        "print(\"Repo cloned and extracted ...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):  # colab installs tf.2.2 on default.\n",
        "    !pip uninstall -y tensorflow\n",
        "    !pip install tensorflow-gpu==1.14\n",
        "!pip install imageio\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "print(\"Installations completed ...\")"
      ],
      "metadata": {
        "id": "1CIDKdXWMhDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import model\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import imageio\n",
        "import sys\n",
        "print(\"\\npwd = \", os.getcwd())\n",
        "\n",
        "from matplotlib import animation, rc, pyplot as plt\n",
        "plt.rcParams['animation.ffmpeg_path'] = '/usr/bin/ffmpeg'\n",
        "from IPython.display import HTML\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"imports success...\")"
      ],
      "metadata": {
        "id": "WLRfK3F8Mjec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_height   = 480 #@param {type:\"integer\"}\n",
        "img_width    = 854 #@param {type:\"integer\"}\n",
        "img_channel  = 3   #@param {type:\"integer\"}\n",
        "num_classes  = 7   #@param {type:\"integer\"}\n",
        "offset_x     = 20  #@param {type:\"integer\"}\n",
        "offset_y     = 11  #@param {type:\"integer\"}\n",
        "VIDEO_NUM = \"01\" #change to train on a different video\n",
        "data_path    = '../drive/MyDrive/surgical_tracking_training_material/video'+VIDEO_NUM+'.mp4'\n",
        "#data_path should contain video from cholec80 dataset\n",
        "\n",
        "print(\"Model and device variables set .. \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF3G-LQvMlwh",
        "outputId": "70e8e743-a1bd-4ae3-e44c-f28dc3b689f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../drive/MyDrive/surgical_tracking_training_material/video01.mp4\n",
            "Model and device variables set .. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#reading training data labels\n",
        "with open(\"../drive/MyDrive/surgical_tracking_training_material/video\" + VIDEO_NUM + \"-tool.txt\", \"r\") as f: #path for ground-truth labels text file\n",
        "  tr_label_matrix = []\n",
        "  label_arr = []\n",
        "  row = 0\n",
        "  col = 0\n",
        "  for line in f:\n",
        "    if(row > 0):\n",
        "      for word in line.split():\n",
        "        if(col > 0):\n",
        "          label_arr.append(float(word)) \n",
        "        col += 1\n",
        "      tr_label_matrix.append(label_arr) # Each row/arrray of labels denotes which collection of tools are present in each frame\n",
        "      label_arr = []\n",
        "      col = 0\n",
        "    row += 1\n",
        " \n"
      ],
      "metadata": {
        "id": "vLE0GABfMouR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "reader = imageio.get_reader(data_path) \n",
        "\n",
        "tr_frames = []\n",
        "for i, frame in enumerate(reader):\n",
        "  if(i % 25 == 0):  #reading every 25th video frame into tr_frames[] (downsampling from 25fps to 1fps)\n",
        "    tr_frames.append(frame)\n"
      ],
      "metadata": {
        "id": "BVTghvadMq1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "img_ph  = tf.placeholder(dtype=tf.float32, shape=[None, None,3], name='inputs') #placeholder for feeding training images\n",
        "x       = tf.expand_dims(img_ph, 0)   \n",
        "x       = tf.image.resize_bilinear(x, size=(480,854))             \n",
        "seek_ph = tf.placeholder(dtype=tf.int64, shape=[None], name='inputs') #placeholder for feeding frame number (used for spatio-temporal calculations)\n",
        "labels_ph = tf.placeholder(dtype=tf.float32, shape=[num_classes], name='labels') #placeholder for feeding training labels\n",
        "network = model.Model(images=x, seek=seek_ph, num_classes=num_classes)\n",
        "logits_float, lhmaps  = network.build_model() #logits collects tool presence probabilities and lhmaps stores location graphs\n",
        "logits_float =  tf.sigmoid(logits_float)\n",
        "logits  = tf.cast(tf.round(logits_float), tf.int32)\n",
        "lhmaps  = lhmaps * tf.cast(logits, tf.float32)\n",
        "\n",
        "\n",
        "print(\"Model loaded successfully...\")"
      ],
      "metadata": {
        "id": "kd806MVPMs0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = -tf.reduce_mean(((labels_ph*tf.log(logits_float + 1e-9)) + ((1-labels_ph) * tf.log(1 - logits_float + 1e-9))) , name='xentropy' )\n",
        "#entropy calculates loss as difference between ground-truth labels (labels_ph) and predicted probabilities (logits)\n",
        "#you can add per class weights to cross_entropy as mentioned in paper\n",
        "\n",
        "#setting training hyperparameters\n",
        "ln_rate =  0.001\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=ln_rate).minimize(cross_entropy)\n",
        "#optimizers will tune weights to reach convergence. The GSD is used in this case\n"
      ],
      "metadata": {
        "id": "gU35faXOMugZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import I\n",
        "\n",
        "\n",
        "#creating a training session\n",
        "import sys\n",
        "\n",
        "sess_config    = tf.ConfigProto(gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9), allow_soft_placement = True, log_device_placement = False) \n",
        "with tf.Session(config=sess_config) as sess:   \n",
        "    sess.run([tf.local_variables_initializer(), tf.global_variables_initializer()])\n",
        "\n",
        "    EPOCHS = 15 #decides number of training cycles\n",
        "    min_frame_count = min(len(tr_frames), len(tr_label_matrix))\n",
        "  \n",
        "\n",
        "   # training loop over the number of epoches\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0;\n",
        "        for i in range(min_frame_count):\n",
        "            x_batch=tr_frames[i]\n",
        "            y_batch=tr_label_matrix[i]\n",
        "\n",
        "            # feeding training data/examples     \n",
        "            _, loss_val = sess.run([optimizer, cross_entropy], feed_dict={img_ph:x_batch , seek_ph:[i], labels_ph:y_batch})\n",
        "            total_loss += loss_val\n",
        "\n",
        "        total_loss /= min_frame_count # computes average loss\n",
        "        print(\"avg loss =\", str(total_loss))\n",
        "\n",
        "        if (epoch % 5 == 0 and epoch > 0): #for every 5 epochs, the model is tested on same video frames\n",
        "          PREDICTIONS    = [] #stores logits(or predictions for each frame)\n",
        "          CLASS_LHMAPS   = [] #stores heat map\n",
        "          seek = 0\n",
        "          #change path below to direct predictions to desired file\n",
        "          with open('../drive/MyDrive/logits_ep'+ str(epoch)+'_of_'+ str(EPOCHS)+'_' + str(min_frame_count) + '.txt', 'w') as f:\n",
        "            for frame in tr_frames:\n",
        "                predict, lhmap = sess.run([logits, lhmaps], feed_dict={img_ph:frame, seek_ph:[seek]}) #evaluates model and collects predictions + heat map\n",
        "                PREDICTIONS.append(predict)\n",
        "                CLASS_LHMAPS.append(lhmap)\n",
        "                if(seek >= min_frame_count):\n",
        "                  break\n",
        "          print(\"Evaluation done after epoch \", epoch)\n"
      ],
      "metadata": {
        "id": "m4hUfRjvMybD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_center_coordinates(lhmap):\n",
        "    coord = np.where(lhmap == lhmap.max()) \n",
        "    cx    = (coord[1][0] * img_width // 107) + offset_x\n",
        "    cy    = (coord[0][0] * img_height // 60) + offset_y\n",
        "    return (cx, cy)\n",
        "\n",
        "def get_box_coordinates(lhmap):\n",
        "    coord = np.where(lhmap>0)\n",
        "    if len(coord[0])>0 and len(coord[1])>0 :\n",
        "        x0 = (coord[1].min() * img_width // 107) - offset_x\n",
        "        x1 = (coord[1].max() * img_width // 107) + offset_x\n",
        "        y0 = (coord[0].min() * img_height // 60) - offset_y\n",
        "        y1 = (coord[0].max() * img_height // 60) + offset_y\n",
        "    else:\n",
        "        x0,x1,y0,y1 = -1,-1,-1,-1\n",
        "    return (x0,y0,x1,y1)\n",
        "\n",
        "\n",
        "# Build animators\n",
        "def build_animators():\n",
        "    BUFFER_BOX_CENTER = []\n",
        "    colors    = [(255,0,0),(255,255,0),(0,0,255),(255,0,255),(255,128,0),(0,255,255),(0,255,0)] \n",
        "    radius    = 28\n",
        "    thickness = 4\n",
        "    reader    = imageio.get_reader(data_path)\n",
        "    fig       = plt.figure()\n",
        "    for k, (img, predict, lhmap) in enumerate(zip(tr_frames, PREDICTIONS, CLASS_LHMAPS)):\n",
        "        img_overlay     = img.copy()\n",
        "        for i in range(num_classes):\n",
        "            cam         = lhmap[0,:,:,i]\n",
        "            x1,y1,x2,y2 = get_box_coordinates(cam)\n",
        "            cx,cy       = get_center_coordinates(cam)\n",
        "            color       = colors[i]\n",
        "            cv2.rectangle(img_overlay, (x1,y1), (x2,y2), color, thickness)\n",
        "            cv2.circle(img_overlay, (cx,cy), radius, color, -1)\n",
        "        cv2.circle(img_overlay, (offset_x,offset_y), radius, (0,0,0), -1)\n",
        "        BUFFER_BOX_CENTER.append([plt.imshow(img_overlay)])\n",
        "    return fig, BUFFER_BOX_CENTER\n",
        "        \n",
        "\n",
        "# Colorizer\n",
        "def cstr(s, color='black'):\n",
        "    return \"<text style=color:{}>{}</text>\".format(color, s)\n",
        "\n",
        "print(\"Model ready to track...\")"
      ],
      "metadata": {
        "id": "tDJd7ICtM1aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, OVERLAY = build_animators()\n",
        "\n",
        "HTML('='*20+\"> [  Tool Colormap:                                       \"\n",
        "           +cstr(\"Grasper\", \"red\") +\" | \"+cstr(\"Bipolar\", \"yellow\") +\"  |  \"+cstr(\"Hook\", \"blue\")+\"  |  \"\n",
        "           +cstr(\"Scissors\", \"violet\")+\"  |  \" +cstr(\"Clipper\", \"orange\") \n",
        "           +\"  |  \"+cstr(\"Irrigator\", \"mouve\") +\"  |  \"+cstr(\"Specimen bag  \", \"green\")+'  ] <'+'='*20 )"
      ],
      "metadata": {
        "id": "io49-UrBM3uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anim = animation.ArtistAnimation(fig, OVERLAY, interval=160, blit=True, repeat_delay=1000)\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "sSbAWrYjM6GJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}